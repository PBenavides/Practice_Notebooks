{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción al Gym:\n",
    "\n",
    "**Por qué necesitamos de un Gym en RL?**\n",
    "\n",
    "Existen dos problemas fundamentales cuando se hacen investigaciones en RL. El primero es que no existe un benchmark final para los algoritmos que salen dentro del campo, a comparación de otras ramas de la Inteligencia Artificial donde se poseen datasets gigantescos para poder comparar el desempeño de múltiples algoritmos. Por eso, OpenAI propone una serie de **environments predeterminados** donde los investigadores pueden probar sus algoritmos bajo las mismas características y de forma fácil ahorrando tiempo y maximizando eficiencia. El seguno motivo es que la **estandarización** de los environment a través de las publicaciones no es estricta, es decir, deja mucha posibilidad a desregularizaciones o mal-especificaciones. Esto lleva a que los papers sean difíciles de replicar, pues los environments no son exactamente replicados. \n",
    "\n",
    "\n",
    "\n",
    "Qué ofrece la librería Gym?:\n",
    "Poder establecer y estandarizar una cantidad diversa de environments para probar nuestros algoritmos de RL: [LISTA DE ENVIRONMENTS](https://gym.openai.com/envs/)\n",
    "\n",
    "Podemos importar esta cantidad de environments, pero para poder mostrar qué conceptos usaremos en primera instancia cogeremos de ejemplo dos juegos: FrozenLake y Taxi.\n",
    "\n",
    "**Frozen Lake**\n",
    "\n",
    "El agente está en un espacio de 4x4, donde tiene que ir de un punto S(Start) hacia un punto G (Goal) sin pasar por los puntos H(Hold) porque en ese caso, pierde.\n",
    "\n",
    "**Taxi**\n",
    "\n",
    "El agente es un taxista, que tiene que recoger pasajeros en una locación, y luego dejarlos en otra. Reciben 20 puntos si lo deja de manera exitosa, pero como cualquier taxista debe de tener en cuenta: el tiempo es dinero. Entonces, el agente perderá 1 punto cada vez que se demore un turno y, además, se le restan 10 puntos por cada recogida ilegal que el agente hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#Cargaremos el environment FrozenLake y lo almacenamos en una variable.\n",
    "lake_env = gym.make('FrozenLake-v0')\n",
    "#Veremos el environment:\n",
    "lake_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el environment Taxi\n",
    "blackjack_env = gym.make('Taxi-v3')\n",
    "blackjack_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cuáles son las características de estos environments?\n",
    "\n",
    "Cada environment viene con sus variables descriptivas que podemos llamar bajo diversos métodos, esto nos será útil para preocuparnos solamente del proceso de optimización y no de la configuración del sistema. A grandes rasgos tenemos:\n",
    "\n",
    "``.observation_space``: El Espacio en el cual se podrá mover el agente.\n",
    "\n",
    "``.action_space``: El diverso número de acciones que puede tomar el agente a través de todo el environment. En su mayoría está encodeado de la forma: {0: Izquierda, 1: Abajo, 2: Derecha, 3:Arriba}\n",
    "\n",
    "``.P``: Nos dará un diccionario con la matriz de transición, en donde cada elemento estará ordenado por [estado] y [acción]. Esto nos es útil porque podemos ver cuáles son las probabilidades de transición, el estado al que lleva esa acción implicada, el reward o pago que obtendremos por hacer esa acción y el Estado de la acción (si es el final o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "[(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "[(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print(lake_env.observation_space)\n",
    "print(lake_env.action_space)\n",
    "print(lake_env.P[0][2])\n",
    "print(lake_env.P[0][0])\n",
    "print(lake_env.P[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n",
      "Discrete(6)\n",
      "[(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print(blackjack_env.observation_space)\n",
    "print(blackjack_env.action_space)\n",
    "print(lake_env.P[0][2])\n",
    "print(lake_env.P[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambientes dinámicos:\n",
    "\n",
    "\n",
    "Los ambientes dinámicos necesitan ser invocados en una iteración, pues es un agente que posee un intervalo de vida que si no se define incurre en error. Además, existen ambientes en 2D y 3D y la complejidad es proporcional a sus dimensiones porque tenemos un mayor espacio de acción. Sin embargo, el tener espacios en 3D posibilita la aplicación de estos algoritmos en robótica!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De la documentación oficial:\n",
    "\n",
    "car_env = gym.make('MountainCarContinuous-v0')\n",
    "observation = car_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    car_env.render()\n",
    "    action = car_env.action_space.sample() #Obtengo una acción random para el agente.\n",
    "    \n",
    "    observation, reward, done, info = car_env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        observation = car_env.reset()\n",
    "        \n",
    "car_env.close() #Siempre tenemos que cerrar el ambiente al finalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase Box es un espacio en R^n que representa un producto cartesiano de los n intervalos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#El Espacio de observación del agente está definido por el Objeto Box. \n",
    "#Recuerda que en espacios discretos esto era Discrete\n",
    "car_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de acción Continuo: (-1.0000 to 1.0000)\n",
      "Rango de rewards: (-inf, inf)\n",
      "Rango de Observación para la dimensión 0: (-1.2000 to 0.6000)\n",
      "Rango de Observación para la dimensión 1: (-0.0700 to 0.0700)\n"
     ]
    }
   ],
   "source": [
    "#Podemos imprimirlo de esta manera\n",
    "print('Espacio de acción Continuo: (%.4f to %.4f)'%(env.action_space.low, env.action_space.high))\n",
    "print('Rango de rewards: %s'%(str(env.reward_range)))\n",
    "for dimension in range(len(env.observation_space.low)):\n",
    "    print('Rango de Observación para la dimensión %d: (%.4f to %.4f)'%\n",
    "         (dimension, env.observation_space.low[dimension], env.observation_space.high[dimension]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
